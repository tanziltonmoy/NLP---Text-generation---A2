{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Language Model using Harry Potter Book Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (4.66.1)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (1.25.2)\n",
      "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.0->torchtext) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.0->torchtext) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.0->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.0->torchtext) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.0->torchtext) (2023.10.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch==2.2.0->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch==2.2.0->torchtext) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (8.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets) (5.10.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: backcall in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.31)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.13.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: asttokens in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.8)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.1.1)\n",
      "Requirement already satisfied: notebook in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter) (7.0.4)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter) (7.8.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter) (6.16.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter) (8.1.2)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab) (0.26.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab) (3.1.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab) (5.3.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab) (2.2.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab) (2.7.3)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.19.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab) (2.25.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab) (0.2.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab) (21.3)\n",
      "Requirement already satisfied: tomli in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab) (2.0.1)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab) (6.2)\n",
      "Requirement already satisfied: traitlets in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab) (5.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from async-lru>=1.0.0->jupyterlab) (4.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.25.0->jupyterlab) (4.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (1.0.3)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.25.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab) (2.1.3)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.4.9)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.7.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.4.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (5.9.2)\n",
      "Requirement already satisfied: overrides in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.17.1)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (2.0.11)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (24.0.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.6.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core->jupyterlab) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-core->jupyterlab) (304)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (2.12.1)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (0.9.14)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (4.19.1)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (0.2.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (3.0.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nbconvert->jupyter) (2.13.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->jupyterlab) (3.0.9)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (1.6.3)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (8.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (1.5.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipykernel->jupyter) (5.9.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets->jupyter) (0.2.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets->jupyter) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipywidgets->jupyter) (3.0.10)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-console->jupyter) (3.0.31)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.25.0->jupyterlab) (1.1.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: stack-data in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.4.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (0.10.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.8.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.18.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.26.16)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: fqdn in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (2.4)\n",
      "Requirement already satisfied: uri-template in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.13)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (1.15.1)\n",
      "Requirement already satisfied: executing in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (1.1.0)\n",
      "Requirement already satisfied: asttokens in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.8)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\asus\\appdata\\roaming\\python\\python310\\site-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install jupyter jupyterlab --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Load data\n",
    "\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "### Content\n",
    "This custom dataset comprises text extracted from all seven Harry Potter books, authored by J.K. Rowling. The collection spans from \"Harry Potter and the Sorcerer's Stone\" to \"Harry Potter and the Deathly Hallows.\" The dataset is meticulously organized into rows, each signifying a chapter across the series, and includes columns for the text content, chapter number, and book number. This structured format allows for in-depth analysis and processing.\n",
    "\n",
    "### Use Case\n",
    "The Harry Potter dataset is a treasure trove for Natural Language Processing (NLP) projects and analyses. It provides a rich basis for a variety of applications, including:\n",
    "- Text Analysis\n",
    "- Sentiment Analysis\n",
    "- Character Network Analysis\n",
    "- Narrative Structure Understanding\n",
    "- Style Analysis\n",
    "- Text Generation\n",
    "\n",
    "Given the dataset's diverse applications, it serves as an excellent resource for data scientists, researchers, and enthusiasts looking to explore the intersections of literature and machine learning.\n",
    "\n",
    "### Source\n",
    "The dataset is derived from a dedicated effort to compile the texts for NLP and text mining purposes, available on GitHub at [ErikaJacobs/Harry-Potter-Text-Mining](https://github.com/ErikaJacobs/Harry-Potter-Text-Mining/). This project provides the groundwork for accessing the rich narrative of the Harry Potter series in a structured data format, enabling various analytical and modeling endeavors.\n",
    "\n",
    "## Acknowledgments\n",
    "Special thanks to the creators and contributors of the [Harry Potter Text Mining project on GitHub](https://github.com/ErikaJacobs/Harry-Potter-Text-Mining/) for compiling and making this dataset accessible. Their work lays a valuable foundation for academic and hobbyist exploration within the realm of text analysis and NLP.\n",
    "\n",
    "## Legal Notice\n",
    "This dataset is intended for educational, research, and non-commercial use. Users of the dataset should ensure to comply with copyright laws and use the data responsibly, respecting the original work of J.K. Rowling. Any commercial use of the data should proceed only with appropriate permissions and adherence to copyright regulations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "file_paths = [\"hp/HPBook{}.txt\".format(i) for i in range(1, 8)]\n",
    "\n",
    "# Read all files into a list of DataFrames\n",
    "dfs = [pd.read_csv(file, sep=\"@\") for file in file_paths]\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Resetting the index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE VANISHING GLASS  Nearly ten years had pass...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THE LETTERS FROM NO ONE  The escape of the Bra...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE KEEPER OF THE KEYS  BOOM. They knocked aga...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DIAGON ALLEY  Harry woke early the next mornin...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Harry remained kneeling at Snape's side, simpl...</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Finally, the truth. Lying with his face presse...</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>He lay facedown, listening to the silence. He ...</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>He was flying facedown on the ground again. Th...</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Autumn seemed to arrive suddenly that year. Th...</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Chapter  Book\n",
       "0    THE BOY WHO LIVED  Mr. and Mrs. Dursley, of nu...        1     1\n",
       "1    THE VANISHING GLASS  Nearly ten years had pass...        2     1\n",
       "2    THE LETTERS FROM NO ONE  The escape of the Bra...        3     1\n",
       "3    THE KEEPER OF THE KEYS  BOOM. They knocked aga...        4     1\n",
       "4    DIAGON ALLEY  Harry woke early the next mornin...        5     1\n",
       "..                                                 ...      ...   ...\n",
       "195  Harry remained kneeling at Snape's side, simpl...       33     7\n",
       "196  Finally, the truth. Lying with his face presse...       34     7\n",
       "197  He lay facedown, listening to the silence. He ...       35     7\n",
       "198  He was flying facedown on the ground again. Th...       36     7\n",
       "199  Autumn seemed to arrive suddenly that year. Th...       37     7\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the DataFrame into training, validation, and test datasets\n",
    "\n",
    "# Training dataset: Contains the first 100 rows of the DataFrame\n",
    "train_dataset = df[0:100]\n",
    "\n",
    "# Validation dataset: Contains rows 100 to 149 (inclusive) of the DataFrame\n",
    "val_dataset = df[100:150]\n",
    "\n",
    "# Test dataset: Contains rows 150 onwards to the end of the DataFrame\n",
    "test_dataset = df[150:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert the original DataFrame df to a Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Convert the train_dataset, val_dataset, and test_dataset DataFrames to Datasets\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "val_dataset = Dataset.from_pandas(val_dataset)\n",
    "test_dataset = Dataset.from_pandas(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text', 'Chapter', 'Book'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a09962d8b3847e891c5033f42ed9403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7054c4fda1c8489eaa31d30db4f6fd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbc6654d47d4fe8a2c1817fb7d70886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['Text'])}  \n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_data, remove_columns=['Text'], fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_data, remove_columns=['Text'], fn_kwargs={'tokenizer': tokenizer})\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_data, remove_columns=['Text'], fn_kwargs={'tokenizer': tokenizer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Chapter', 'Book', 'tokens'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens of a specific example in the training dataset\n",
    "print(tokenized_train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "# Build vocabulary from the tokenized training dataset\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_train_dataset['tokens'], min_freq=3)\n",
    "\n",
    "# Insert special tokens '<unk>' and '<eos>'\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "\n",
    "# Set the default index for unknown tokens\n",
    "vocab.set_default_index(vocab['<unk>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8042\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', '.', ',', 'the', \"'\", '\\\\', 'and', 'to', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches) #view vs. reshape (whether data is contiguous)\n",
    "    return data #[batch size, seq len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data = get_data(tokenized_train_dataset, vocab, batch_size)\n",
    "valid_data = get_data(tokenized_val_dataset, vocab, batch_size)\n",
    "test_data  = get_data(tokenized_train_dataset, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 4894])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 33,271,658 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model      = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, seq len]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 10m 5s\n",
      "\tTrain Perplexity: 547.609\n",
      "\tValid Perplexity: 343.123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 9m 53s\n",
      "\tTrain Perplexity: 285.497\n",
      "\tValid Perplexity: 181.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 9m 48s\n",
      "\tTrain Perplexity: 161.361\n",
      "\tValid Perplexity: 126.969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 9m 58s\n",
      "\tTrain Perplexity: 119.602\n",
      "\tValid Perplexity: 105.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 10m 33s\n",
      "\tTrain Perplexity: 100.266\n",
      "\tValid Perplexity: 95.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 10m 37s\n",
      "\tTrain Perplexity: 88.961\n",
      "\tValid Perplexity: 89.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 10m 32s\n",
      "\tTrain Perplexity: 80.887\n",
      "\tValid Perplexity: 83.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Epoch Time: 10m 29s\n",
      "\tTrain Perplexity: 74.716\n",
      "\tValid Perplexity: 79.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 10m 7s\n",
      "\tTrain Perplexity: 70.043\n",
      "\tValid Perplexity: 75.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Epoch Time: 10m 1s\n",
      "\tTrain Perplexity: 65.866\n",
      "\tValid Perplexity: 73.283\n",
      "Total Time: 102m 9s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_epochs = 10\n",
    "seq_len = 50  # Decoding length\n",
    "clip = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()  \n",
    "\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {int(epoch_mins)}m {int(epoch_secs)}s')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n",
    "\n",
    "total_end_time = time.time()\n",
    "total_mins, total_secs = divmod(total_end_time - total_start_time, 60)\n",
    "print(f'Total Time: {int(total_mins)}m {int(total_secs)}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Model Architecture \n",
    "\n",
    "\n",
    "## Data Preprocessing Steps\n",
    "\n",
    "- **Data Splitting**: The dataset was divided into training, validation, and test sets to ensure the model is evaluated on unseen data effectively.\n",
    "- **Dataset Conversion**: Pandas DataFrames were converted into `Dataset` objects for more efficient data manipulation, leveraging the capabilities of the `datasets` library.\n",
    "- **Tokenization**: A tokenizer was employed to break down the text into tokens, which serve as the basic units for model training. This process is crucial for understanding and processing the raw text data.\n",
    "- **Vocabulary Building**: A vocabulary was created from the tokenized training data, mapping each unique token to an index. Special tokens for unknown words (`<unk>`) and end-of-sequence markers (`<eos>`) were included to handle out-of-vocabulary words and sequence endings, respectively.\n",
    "- **Numericalization and Batching**: Text tokens were transformed into numerical indices based on the vocabulary, and the data was organized into batches. This step is essential for preparing the data for input into the machine learning model.\n",
    "\n",
    "## Model Architecture and Training\n",
    "\n",
    "### Architecture\n",
    "- The model architecture is based on LSTM (Long Short-Term Memory) networks, known for their effectiveness in handling sequences and long-term dependencies in text data.\n",
    "- It includes an **embedding layer** to transform token indices into dense vector representations, an **LSTM layer** for processing sequences, a **dropout layer** for regularization, and a **fully connected layer** to output predictions across the vocabulary.\n",
    "\n",
    "### Training\n",
    "- The model was trained on batches of data, using cross-entropy loss to calculate the difference between predicted and actual next tokens in sequences.\n",
    "- **Gradient clipping** was applied to prevent exploding gradients, a common issue in training deep neural networks.\n",
    "- **Learning rate adjustments** were made based on the performance on the validation set, optimizing the training process and model's ability to generalize.\n",
    "\n",
    "### Evaluation\n",
    "- The model's performance was evaluated using perplexity on both the validation and test datasets. Perplexity measures the model's uncertainty in predicting the next token, with lower values indicating better performance.\n",
    "- This metric ensures the model generalizes well to new, unseen data, an essential aspect of effective language modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 49.730\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the modelâ€™s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "harry potter is going to be a lot of magic , and he was a lot of magic in the dark arts . he was looking over for the map and saw the\n",
      "\n",
      "0.7\n",
      "harry potter is going to be caught in his face , and he had never forgotten that even he was going to be bewitched . \\you don ' t want to see that\n",
      "\n",
      "0.75\n",
      "harry potter is going to be caught in his face , and he had never forgotten that even he was going to be bewitched . \\you don ' t want to see that\n",
      "\n",
      "0.8\n",
      "harry potter is going to be caught in his face , wondering , \\ said harry , even surprised that harry could come back . \\you don ' t want to see that\n",
      "\n",
      "1.0\n",
      "harry potter is going to send him in dangerous face , wondering , \\ said harry , even surprised his bill looked flooding . \\you took him from each day , he was\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Harry Potter is'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming lstm dictionary is correctly defined as shown previously\n",
    "lstm = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'emb_dim': emb_dim,\n",
    "    'hid_dim': hid_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout_rate': dropout_rate,\n",
    "    'tokenizer': tokenizer,\n",
    "    'vocab': vocab\n",
    "}\n",
    "\n",
    "# Correctly save the lstm dictionary to a pickle file\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(lstm, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
